## 1
See annotation.md

## 2
See question 4.

## 3
Resilience patterns, also known as resilience design patterns or resilience engineering patterns, are a set of best practices and strategies for designing and building software systems that are robust and able to gracefully handle failures, disruptions, and unexpected conditions.

Retry: Retry patterns involve automatically retrying a failed operation with the hope that the issue causing the failure is transient and will be resolved after a short delay. 

Circuit Breaker: he circuit breaker pattern helps prevent further execution of a failing operation for a predetermined period. If a certain threshold of failures is reached, the circuit breaker "trips" and blocks further attempts. This helps prevent overloading systems that are already struggling.

Timeout: Setting timeouts on operations ensures that they don't take too long to complete. If an operation exceeds the timeout period, it can be canceled or treated as failed, preventing potential resource exhaustion.

Fallback: Fallback patterns involve providing an alternative response or behavior when a primary operation fails. For example, using cached data or a default value instead of returning an error.

## 4

# Write main components of Microservices
* Services: difference actual business services
* Api getway: entry point of the system, find which service to handle the request, do authentication, load balancing...
* eureka: Services registry. Register services ip enables services to find each other.
* cloud config: a place to hold all config online.
* message queue: kafka, data streaming between services
* Docker: container to run the services
* circuit breaker: hystrix

# What are the benefits and drawbacks of Microservices?
Advantages: scalbility, technial freedom, fault tolerant, decoupling, easy to deploy, develop fast...

Disadvantage: more complex, data management

# What issues are generally solved by spring clouds?
Service Discovery issues, Load balancing issues.

# Explain how independent microservices communicate with each other.
* HTTP/REST with JSON or binary protocol for request-response 
* Websockets for streaming.  
* A broker or server program that uses advanced routing algorithms. 

## 5

Where are load balancer placed:
In between the client application/user and the server
In between the server and the application/job servers
In between the application servers and the cache servers
In between the cache servers the database servers

Types of load balancers:
Software Load Balancers in Clients,Software Load Balancers in Services,Hardware Load Balancers.

# How to do load balancing in mircroservices?
Multiple Services:

In a microservices architecture, different services handle specific tasks or functionalities, such as user authentication, data processing, or database interactions.

Incoming Requests:

Users or external systems send requests to access these services.
As the number of requests grows, the challenge is to evenly distribute these requests among the available service instances.

Load Balancer:

A load balancer sits between users/clients and the services.
Its role is to receive incoming requests and decide which service instance should handle each request.
Distributing Requests:

The load balancer uses various algorithms (e.g., round-robin, least connections) to evenly distribute requests across different service instances.

Load balancing can be achieved using various tools and techniques such as hardware or software load balancers, service meshes, reverse proxies, and container orchestration platforms.




## 6
Set Up Eureka Server:

Start by setting up a Eureka server, which will act as the registry for service instances.
Configure the Eureka server with properties like port number, hostname, and any other required settings.
Run the Eureka server application.

Configure Eureka Clients:

Each microservice that wants to register itself with Eureka should be configured as a Eureka client.
Add the Eureka client dependency to your microservice's build configuration (e.g., Maven or Gradle).
Configure the Eureka client with properties like application name, Eureka server URL, and other optional settings.

Register Services:

In your microservice code, configure it as a Eureka client by adding the appropriate annotations or configuration settings.
When the microservice starts, it will automatically register itself with the Eureka server.

Discover Services:

To discover other services, a microservice can use the Eureka client to query the Eureka server for registered instances of a specific service.
Eureka provides a REST API endpoint that can be used to retrieve a list of service instances.

## 7
Producer:

Producers are responsible for sending data or events to Kafka topics.
They publish records (messages) to Kafka topics, which are then stored in partitions.

Broker:

Brokers are the Kafka servers that store and manage the data.
Each broker can manage one or more partitions of Kafka topics.
Brokers handle read and write requests from producers and consumers.

Topic:

A topic is a logical channel or category to which records are published.
Producers publish records to topics, and consumers subscribe to topics to consume the records.

Partition:

Topics can be divided into partitions to allow parallel processing and scalability.
Each partition is a linearly ordered sequence of records.
Partitions enable Kafka to distribute data and parallelize read and write operations.

Consumer:

Consumers are applications that read data from Kafka topics.
They subscribe to one or more topics and consume records from the partitions of those topics.

Consumer Group:

Consumer groups allow multiple consumers to work together to consume records from a topic.
Each consumer within a group processes records from one or more partitions of the topic.
Kafka ensures that each record is consumed by only one consumer within the group.

Zookeeper:

Although Kafka is moving towards removing its dependency on Zookeeper, it has historically used Zookeeper for maintaining cluster metadata, leader election, and coordination.
Zookeeper ensures the health and availability of Kafka brokers.

Offset:

Each record in a partition is assigned a unique offset, which is a sequential identifier.
Consumers use offsets to keep track of the records they have consumed.
Offsets allow consumers to resume reading from where they left off in case of failures.

## 8
Partition:

Topics can be divided into partitions to allow parallel processing and scalability.
Each partition is a linearly ordered sequence of records.
Partitions enable Kafka to distribute data and parallelize read and write operations.

## 9
Zookeeper is used for maintaining cluster metadata, leader election, and coordination.
Zookeeper ensures the health and availability of Kafka brokers.

## 10
No

## 11
Leader:

Each partition of a topic has one designated leader replica.
The leader replica is responsible for handling all read and write requests for the partition.
Producers send records to the leader replica, and consumers read records from the leader replica.
The leader replica maintains the in-sync replicas (followers) and coordinates the replication process.


Follower:

Each partition can have multiple follower replicas.
Follower replicas replicate data from the leader replica to stay in sync.
Follower replicas do not handle client requests directly; they simply replicate data from the leader.
Follower replicas are important for fault tolerance: if the leader replica becomes unavailable, one of the follower replicas can be promoted to leader to ensure the partition's availability.

## 12
Topic replication is a crucial feature in Apache Kafka that contributes to the platform's durability, fault tolerance, and high availability. Replication involves maintaining multiple copies of the same data across different brokers in a Kafka cluster.

In Kafka, the concept of "In-Sync Replicas" (ISR) is important in the context of replication. The ISR is a subset of replicas for a partition that are currently caught up with the leader replica. ISR is relevant because it ensures that data consistency is maintained even in the presence of network or broker failures.
The ISR concept guarantees that Kafka maintains strong data consistency.

## 13
Consumer Group:

A consumer group is a logical grouping of one or more consumer instances that work together to consume records from Kafka topics.
Each partition within a topic can be consumed by only one consumer instance within a consumer group. This ensures that every record in a partition is processed by only one consumer, preventing duplication.
A consumer group allows parallel consumption of records from multiple partitions across different topics.

consumer groups are essential for achieving parallel processing, scalability, load balancing, fault tolerance, and ordering guarantees in Kafka-based systems. They allow multiple consumers to collaborate effectively while consuming records from Kafka topics, making them a foundational concept for building real-time data processing and streaming applications.

## 14
Use zookeeper

## 15
Online retail: walmart
Meidia streaming: spotify

## 16
In Apache Kafka, a partition key is a value associated with a message that determines which partition within a Kafka topic the message will be sent to. Each topic in Kafka is divided into one or more partitions, and messages are distributed among these partitions based on their partition keys.

Messages with the same partition key are guaranteed to be written to the same partition, ensuring that related messages are stored together.
This property is essential for maintaining order and consistency within a partition. Messages within a partition are guaranteed to be stored in the order they were produced.

## 17
Scalability and Parallelism:

Kafka topics can be divided into multiple partitions.
Each partition acts as an independent unit of storage and processing.
Dividing topics into partitions allows Kafka to achieve high throughput by processing multiple records in parallel.

Distribution of Data:

Partitions distribute data across brokers in a Kafka cluster.
Each broker can host multiple partitions from various topics.
This distribution ensures that the data is spread evenly across the cluster, preventing hotspots and balancing the load.

Retaining Order:

Kafka maintains the order of records within each partition.
This ordering guarantee is crucial for use cases that require maintaining event sequence, such as event sourcing and log aggregation.

## 18
RabbitMQ and Apache Kafka allow producers to send messages to consumers. Producers are applications that publish information, while consumers are applications that subscribe to and process information.

Producers and consumers interact differently in RabbitMQ and Kafka. In RabbitMQ, the producer sends and monitors if the message reaches the intended consumer. On the other hand, Kafka producers publish messages to the queue regardless of whether consumers have retrieved them.

Message consumption:
In RabbitMQ, the broker ensures that consumers receive the message. The consumer application takes a passive role and waits for the RabbitMQ broker to push the message into the queue. For example, a banking application might wait for SMS alerts from the central transaction processing software.

Kafka consumers, however, are more proactive in reading and tracking information. As messages are added to physical log files, Kafka consumers keep track of the last message they've read and update their offset tracker accordingly. An offset tracker is a counter that increments after reading a message. With Kafka, the producer is not aware of message retrieval by consumers. 

Message priority:
RabbitMQ brokers allow producer software to escalate certain messages by using the priority queue. Instead of sending messages with the first in, first out order, the broker processes higher priority messages ahead of normal messages. For example, a retail application might queue sales transactions every hour. However, if the system administrator issues a priority backup database message, the broker sends it immediately.

Unlike RabbitMQ, Apache Kafka doesn't support priority queues. It treats all messages as equal when distributing them to their respective partitions. 

Message deletion:
A RabbitMQ broker routes the message to the destination queue. Once read, the consumer sends an acknowledgement (ACK) reply to the broker, which then deletes the message from the queue.

Unlike RabbitMQ, Apache Kafka appends the message to a log file, which remains until its retention period expires. That way, consumers can reprocess streamed data at any time within the stipulated period.

## 19
Kafka maintains strict ordering of records within each partition.

Kafka guarantees that messages sent to topics are persisted to disk.
This ensures that data is not lost, even in the event of broker failures.

Kafka provides support for exactly-once processing semantics, ensuring that records are neither lost nor duplicated during consumption.

## 20
If partitions are not evenly distributed across brokers, some brokers might end up handling a disproportionate amount of data and requests, while others are underutilized.

Tools like Kafka Manager, Kafka Monitor, and Confluent Control Center can assist in monitoring and managing cluster balance.

## 21
both.

## 22
Post.

## 23
None.

## 24
None.

## 25
Each message in a Kafka partition is assigned a unique offset.
Offsets are sequential integers that start from 0 and increase incrementally.
Offsets are specific to a partition and allow messages within the same partition to be ordered based on their offsets.

